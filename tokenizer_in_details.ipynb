{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzEu63gnQKypQ2xfIg5JrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somnathsingh31/inside_LLM_Architecture/blob/main/tokenizer_in_details.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xZi6XLTNoK8e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xkUtC258nPao"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'bert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzJDHpDln_0J",
        "outputId": "b020551e-04f4-4863-ae90-ff5b006884f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_input = \"This is my village. It is haven with lush fields, vibrant culture, traditional houses, warm community, ancient temples, and simple, self-sufficient living.\""
      ],
      "metadata": {
        "id": "h91N4PZqoVcn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = tokenizer(raw_input, return_tensors='pt')"
      ],
      "metadata": {
        "id": "s0L4tjDFotF-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXGb7RCdo1_M",
        "outputId": "c05ad79a-80f9-47fc-efdc-46d5d0f00269"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1188,  1110,  1139,  1491,   119,  1135,  1110,  3983,  1114,\n",
            "         19302,  3872,   117, 18652,  2754,   117,  2361,  2725,   117,  3258,\n",
            "          1661,   117,  2890,  8433,   117,  1105,  3014,   117,  2191,   118,\n",
            "          6664,  1690,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Attention masks* are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
      ],
      "metadata": {
        "id": "wEU-D3uDv6hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns token id corrsponding to given raw text.\n",
        "\n",
        "There are two steps invovled: i). splitting text into tokens ii). convert tokens into numbers or token IDs"
      ],
      "metadata": {
        "id": "N-LClPXOo64w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'bert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "pXB5yEgZo4Hs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first step: splitting text into tokens\n",
        "tokens = tokenizer.tokenize(raw_input)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0v5Y5qkpiLz",
        "outputId": "557289e0-a6c8-4f23-ec09-bf785c99a900"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'my', 'village', '.', 'It', 'is', 'haven', 'with', 'lush', 'fields', ',', 'vibrant', 'culture', ',', 'traditional', 'houses', ',', 'warm', 'community', ',', 'ancient', 'temples', ',', 'and', 'simple', ',', 'self', '-', 'sufficient', 'living', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second step: convert tokens into numbers or token IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLXzyxYNpvMc",
        "outputId": "972722a4-e264-433a-abe4-96336872b8db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1188, 1110, 1139, 1491, 119, 1135, 1110, 3983, 1114, 19302, 3872, 117, 18652, 2754, 117, 2361, 2725, 117, 3258, 1661, 117, 2890, 8433, 117, 1105, 3014, 117, 2191, 118, 6664, 1690, 119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding is going the other way around: from vocabulary indices, we want to get a string**"
      ],
      "metadata": {
        "id": "IB9cHifxqtTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode(token_ids)\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pe07V0mqEv-",
        "outputId": "f10c3956-5b1e-4439-904a-8123d8dcf174"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is my village. It is haven with lush fields, vibrant culture, traditional houses, warm community, ancient temples, and simple, self - sufficient living.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwliGGZDq673"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Note:*** The decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence."
      ],
      "metadata": {
        "id": "HIambyY2rJCx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-vVIQLYrL0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feeding token_id directly to the model will result in an IndexError because the model expects input data with a batch dimension (i.e., a 2D tensor) where the first dimension represents the batch size."
      ],
      "metadata": {
        "id": "EN0gxlXZsUOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "mcHEDGxat9P_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of tokenized input directly using tokenize(raw_input, return_tensors='pt): \", tokenized_input[\"input_ids\"].size())\n",
        "print(\"Size of tokenized input using tokenizer.convert_tokens_to_ids(tokens): \", torch.tensor(token_ids).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHy__cLJs6sK",
        "outputId": "1597632b-833a-4c13-c0c3-60c5e39311de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of tokenized input directly using tokenize(raw_input, return_tensors='pt):  torch.Size([1, 34])\n",
            "Size of tokenized input using tokenizer.convert_tokens_to_ids(tokens):  torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solve this by converting into tensors as below\n",
        "token_ids_input = torch.tensor([token_ids])\n",
        "print(\"size: \", token_ids_input.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9Bb5tt2tazF",
        "outputId": "e0c82910-9513-4abf-8a34-0e765832de11"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size:  torch.Size([1, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this we can feed to model"
      ],
      "metadata": {
        "id": "EUC_YT8DufgW"
      }
    }
  ]
}